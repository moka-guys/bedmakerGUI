diff --git a/app/bed_generator/api.py b/app/bed_generator/api.py
index 30317e4..b159d70 100644
--- a/app/bed_generator/api.py
+++ b/app/bed_generator/api.py
@@ -17,6 +17,9 @@ from typing import Dict, List, Optional
 import time
 import logging
 from dataclasses import dataclass
+import aiohttp
+import asyncio
+from .debug import log_execution_time, logger
 
 logging.basicConfig(level=logging.INFO)
 logger = logging.getLogger(__name__)
@@ -91,6 +94,53 @@ class ApiClient:
     def get_panelapp_data(cls, url: str) -> Optional[Dict]:
         return cls.make_api_request(url)
 
+class AsyncApiClient:
+    def __init__(self):
+        self.session = None
+        self.ENSEMBL_GRCh38_URL = "https://rest.ensembl.org"
+        self.ENSEMBL_GRCh37_URL = "https://grch37.rest.ensembl.org"
+        self.TARK_API_URL = "https://tark.ensembl.org/api/"
+
+    async def __aenter__(self):
+        self.session = aiohttp.ClientSession()
+        return self
+
+    async def __aexit__(self, exc_type, exc_val, exc_tb):
+        if self.session:
+            await self.session.close()
+
+    async def make_request(self, url: str, params: Optional[Dict] = None) -> Optional[Dict]:
+        try:
+            logger.debug(f"Making request to {url} with params {params}")
+            async with self.session.get(url, params=params) as response:
+                if response.status == 429:  # Rate limit
+                    logger.warning("Rate limited, waiting 2 seconds")
+                    await asyncio.sleep(2)
+                    return await self.make_request(url, params)
+                response.raise_for_status()
+                return await response.json()
+        except Exception as e:
+            logger.error(f"API request failed: {str(e)}")
+            return None
+
+    async def fetch_variant_info(self, identifier: str, assembly: str) -> Optional[Dict]:
+        base_url = self.ENSEMBL_GRCh38_URL if assembly == 'GRCh38' else self.ENSEMBL_GRCh37_URL
+        url = f"{base_url}/vep/human/id/{identifier}"
+        logger.debug(f"Fetching variant info for {identifier}")
+        return await self.make_request(url)
+
+    async def fetch_transcript_info(self, identifier: str, assembly: str) -> Optional[Dict]:
+        url = f"{self.TARK_API_URL}transcript/search/"
+        params = {
+            'identifier_field': identifier.split('.')[0],
+            'expand': 'exons,genes',
+            'assembly_name': 'GRCh38' if assembly == 'GRCh38' else 'GRCh37'
+        }
+        logger.debug(f"Fetching transcript info for {identifier} with params: {params}")
+        response = await self.make_request(url, params)
+        logger.debug(f"Raw API response for {identifier}: {response}")
+        return response
+
 # Main functions
 @dataclass
 class VariantInfo:
diff --git a/app/bed_generator/utils.py b/app/bed_generator/utils.py
index 3baeb3c..3fa0c96 100644
--- a/app/bed_generator/utils.py
+++ b/app/bed_generator/utils.py
@@ -21,8 +21,11 @@ import json
 from flask import current_app
 from app.models import Settings
 from typing import List, Dict, Tuple, Any, Optional
-from .api import fetch_variant_info, fetch_data_from_tark, fetch_coordinate_info, fetch_genes_for_panel
+from .api import AsyncApiClient, fetch_variant_info, fetch_data_from_tark, fetch_coordinate_info, fetch_genes_for_panel
 import datetime
+import asyncio
+import aiohttp
+from .debug import log_execution_time, logger
 
 # Constants
 PANELS_JSON_PATH = os.path.join(os.path.dirname(__file__), 'panels.json')
@@ -32,106 +35,115 @@ def load_settings():
     settings = Settings.get_settings()
     return settings.to_dict()
 
-def process_identifiers(identifiers: List[str], assembly: str, include_5utr: bool, include_3utr: bool) -> Tuple[List[Dict[str, Any]], List[str]]:
-    """
-    Processes a list of genetic identifiers, fetching data and applying UTR and padding adjustments.
-    """
+async def process_identifiers_async(identifiers: List[str], assembly: str, include_5utr: bool, include_3utr: bool) -> Tuple[List[Dict[str, Any]], List[str]]:
+    logger.debug(f"Processing {len(identifiers)} identifiers asynchronously")
     results = []
     no_data_identifiers = []
     
-    with concurrent.futures.ThreadPoolExecutor(max_workers=10) as executor:
-        future_to_id = {}
-        for identifier in identifiers:
-            if re.match(r'^RS\d+$', identifier, re.IGNORECASE):
-                print(f"Processing rsID: {identifier}")
-                future_to_id[executor.submit(fetch_variant_info, identifier, assembly)] = identifier
-            else:
-                future_to_id[executor.submit(fetch_data_from_tark, identifier, assembly)] = identifier
-        
-        for future in concurrent.futures.as_completed(future_to_id):
-            identifier = future_to_id[future]
-            try:
-                data = future.result()
-                if data:
-                    if isinstance(data, list):
-                        # If we're looking for GRCh37 and no data found, try using MANE SELECT stable_id
-                        if assembly == 'GRCh37' and not any(d.get('assembly_name') == 'GRCh37' for d in data):
-                            mane_select = next((d for d in data if d.get('mane_transcript_type') == 'MANE SELECT'), None)
-                            if mane_select and mane_select.get('stable_id'):
-                                print(f"Attempting secondary lookup using MANE SELECT stable_id: {mane_select['stable_id']}")
-                                secondary_data = fetch_data_from_tark(mane_select['stable_id'], assembly)
-                                if secondary_data:
-                                    # Preserve MANE status from original data
-                                    if isinstance(secondary_data, list):
-                                        for sd in secondary_data:
-                                            sd['mane_transcript_type'] = mane_select.get('mane_transcript_type')
-                                            sd['warning'] = {
-                                                'type': 'version_specified',
-                                                'message': f"Using GRCh37 version of MANE Select transcript"
-                                            }
-                                    data = secondary_data
-
-                        # Process TARK data
-                        for r in data:
-                            if r is None:
-                                continue
-                            processed_r = process_tark_data(r, include_5utr, include_3utr)
-                            if processed_r:
-                                results.append(processed_r)
-                    else:
-                        # Handle VariantInfo dataclass
-                        variant_dict = {
-                            'loc_region': data.loc_region,
-                            'loc_start': data.loc_start,
-                            'loc_end': data.loc_end,
-                            'gene': data.gene,
-                            'accession': data.accession,
-                            'entrez_id': data.entrez_id,
-                            'transcript_biotype': data.rsid,
-                            'most_severe_consequence': data.most_severe_consequence,
-                            'allele_string': data.allele_string,
-                            'original_loc_start': data.loc_start,
-                            'original_loc_end': data.loc_end,
-                            'rsid': data.rsid,
-                            'is_snp': True,  # Add flag to identify SNP entries
-                            'mane_transcript_type': None  # Explicitly set to None for SNPs
-                        }
-                        results.append(variant_dict)
+    try:
+        async with AsyncApiClient() as client:
+            tasks = []
+            for identifier in identifiers:
+                logger.debug(f"Setting up task for identifier: {identifier}")
+                if re.match(r'^RS\d+$', identifier, re.IGNORECASE):
+                    logger.debug(f"Queueing rsID: {identifier}")
+                    task = client.fetch_variant_info(identifier, assembly)
                 else:
-                    no_data_identifiers.append(identifier)
-                    print(f"No data found for {identifier}")
-            except Exception as e:
-                print(f"Error processing identifier {identifier}: {e}")
-                no_data_identifiers.append(identifier)
-    
+                    logger.debug(f"Queueing transcript: {identifier}")
+                    task = client.fetch_transcript_info(identifier, assembly)
+                tasks.append((identifier, task))
+
+            batch_size = 5
+            for i in range(0, len(tasks), batch_size):
+                batch = tasks[i:i + batch_size]
+                logger.debug(f"Processing batch {i//batch_size + 1} of {len(tasks)//batch_size + 1}")
+                
+                try:
+                    logger.debug(f"Awaiting batch results for {[id for id, _ in batch]}")
+                    batch_results = await asyncio.gather(*(task for _, task in batch), return_exceptions=True)
+                    logger.debug(f"Batch {i//batch_size + 1} completed with {len(batch_results)} results")
+                    
+                    for (identifier, _), data in zip(batch, batch_results):
+                        if isinstance(data, Exception):
+                            logger.error(f"Error processing {identifier}: {str(data)}")
+                            no_data_identifiers.append(identifier)
+                            continue
+                        
+                        logger.debug(f"Got API response for {identifier}: {bool(data)}")
+                        if data:
+                            logger.debug(f"Processing data for {identifier}")
+                            processed = process_single_identifier(identifier, data, assembly, include_5utr, include_3utr)
+                            if processed:
+                                logger.debug(f"Successfully processed {identifier}")
+                                results.extend([processed] if not isinstance(processed, list) else processed)
+                            else:
+                                logger.warning(f"No valid data after processing {identifier}")
+                                no_data_identifiers.append(identifier)
+                        else:
+                            logger.warning(f"No data found for {identifier}")
+                            no_data_identifiers.append(identifier)
+                except Exception as e:
+                    logger.error(f"Error processing batch: {str(e)}", exc_info=True)
+                    continue
+
+    except Exception as e:
+        logger.error(f"Error in process_identifiers_async: {str(e)}", exc_info=True)
+        raise
+
+    logger.info(f"Completed processing with {len(results)} results and {len(no_data_identifiers)} failures")
+    logger.debug(f"Results: {results}")
+    logger.debug(f"No data identifiers: {no_data_identifiers}")
     return results, no_data_identifiers
 
+def process_identifiers(identifiers: List[str], assembly: str, include_5utr: bool, include_3utr: bool) -> Tuple[List[Dict[str, Any]], List[str]]:
+    """Synchronous wrapper for async function"""
+    logger.debug(f"Starting synchronous wrapper for {len(identifiers)} identifiers")
+    try:
+        return asyncio.run(process_identifiers_async(identifiers, assembly, include_5utr, include_3utr))
+    except Exception as e:
+        logger.error(f"Error in process_identifiers: {str(e)}")
+        raise
+
 def process_tark_data(r: Dict[str, Any], include_5utr: bool, include_3utr: bool) -> Optional[Dict[str, Any]]:
+    """Process TARK data and apply UTR adjustments."""
+    logger.debug(f"Processing TARK data with UTR settings - 5': {include_5utr}, 3': {include_3utr}")
+    
     if r is None or 'loc_start' not in r or 'loc_end' not in r:
+        logger.warning("Invalid TARK data: missing required fields")
         return None
 
-    strand = r.get('loc_strand', 1)
-    
-    # Store the full coordinates and UTR positions
-    r['full_loc_start'] = r['loc_start']
-    r['full_loc_end'] = r['loc_end']
-    r['five_prime_utr_end'] = r.get('five_prime_utr', {}).get('end')
-    r['three_prime_utr_start'] = r.get('three_prime_utr', {}).get('start')
-    r['strand'] = strand
+    try:
+        strand = r.get('strand', 1)
+        
+        # Store the full coordinates and UTR positions
+        result = r.copy()
+        result['full_loc_start'] = r['loc_start']
+        result['full_loc_end'] = r['loc_end']
+        result['five_prime_utr_end'] = r.get('five_prime_utr', {}).get('end')
+        result['three_prime_utr_start'] = r.get('three_prime_utr', {}).get('start')
+        result['strand'] = strand
 
-    # Apply initial UTR adjustments
-    if strand == 1:  # Positive strand
-        if not include_5utr and r['five_prime_utr_end']:
-            r['loc_start'] = max(r['loc_start'], r['five_prime_utr_end'])
-        if not include_3utr and r['three_prime_utr_start']:
-            r['loc_end'] = min(r['loc_end'], r['three_prime_utr_start'])
-    else:  # Negative strand
-        if not include_5utr and r['five_prime_utr_end']:
-            r['loc_end'] = min(r['loc_end'], r['five_prime_utr_end'])
-        if not include_3utr and r['three_prime_utr_start']:
-            r['loc_start'] = max(r['loc_start'], r['three_prime_utr_start'])
-    
-    return r
+        logger.debug(f"Full coordinates: {result['full_loc_start']}-{result['full_loc_end']}")
+        logger.debug(f"UTR positions - 5' end: {result['five_prime_utr_end']}, 3' start: {result['three_prime_utr_start']}")
+
+        # Apply UTR adjustments based on strand
+        if strand == 1:  # Positive strand
+            if not include_5utr and result['five_prime_utr_end']:
+                result['loc_start'] = max(result['loc_start'], result['five_prime_utr_end'])
+            if not include_3utr and result['three_prime_utr_start']:
+                result['loc_end'] = min(result['loc_end'], result['three_prime_utr_start'])
+        else:  # Negative strand
+            if not include_5utr and result['five_prime_utr_end']:
+                result['loc_end'] = min(result['loc_end'], result['five_prime_utr_end'])
+            if not include_3utr and result['three_prime_utr_start']:
+                result['loc_start'] = max(result['loc_start'], result['three_prime_utr_start'])
+
+        logger.debug(f"Adjusted coordinates: {result['loc_start']}-{result['loc_end']}")
+        return result
+
+    except Exception as e:
+        logger.error(f"Error in process_tark_data: {str(e)}", exc_info=True)
+        return None
 
 def process_coordinates(coordinates: List[str], assembly: str = 'GRCh38') -> List[Dict[str, Any]]:
     """
@@ -304,4 +316,39 @@ def standardize_result(result: Dict[str, Any]) -> Dict[str, Any]:
         standard_result['full_loc_start'] = result['full_loc_start']
         standard_result['full_loc_end'] = result['full_loc_end']
     
-    return standard_result
\ No newline at end of file
+    return standard_result
+
+def process_single_identifier(identifier: str, data: List[Dict[str, Any]], assembly: str, include_5utr: bool, include_3utr: bool) -> Optional[Dict[str, Any]]:
+    """Process a single identifier's data."""
+    logger.debug(f"Processing single identifier: {identifier}")
+    
+    try:
+        if isinstance(data, list):
+            # First, process the raw TARK data through process_transcripts
+            from .api import process_transcripts
+            processed_transcripts = process_transcripts(data, identifier)
+            
+            if not processed_transcripts:
+                logger.warning(f"No transcripts processed for {identifier}")
+                return None
+            
+            # Then process UTRs for each transcript
+            for transcript in processed_transcripts:
+                # Add strand information before UTR processing
+                transcript['strand'] = transcript.get('loc_strand', 1)
+                
+                # Process UTRs
+                processed = process_tark_data(transcript, include_5utr, include_3utr)
+                if processed:
+                    logger.debug(f"Successfully processed transcript {transcript.get('accession')} for {identifier}")
+                    return processed
+            
+            logger.warning(f"No valid transcripts after UTR processing for {identifier}")
+            return None
+        else:
+            logger.warning(f"Unexpected data format for {identifier}")
+            return None
+            
+    except Exception as e:
+        logger.error(f"Error processing identifier {identifier}: {str(e)}", exc_info=True)
+        return None
\ No newline at end of file
